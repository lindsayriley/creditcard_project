{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "4eb685aa-e65a-47ac-834a-83b0ee6d33e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pandas\n",
    "#!pip install numpy\n",
    "#!pip install matplotlib\n",
    "#!pip install scikit-learn\n",
    "#!pip install xlrd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2dcccad-9e92-4a46-967d-8a5dff924ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import xlrd\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2405a7bf-4dc0-41d6-9f60-38cefd9e4ea0",
   "metadata": {},
   "source": [
    "# Import Data\n",
    "\n",
    "I am importing the data from Excel, then reporting a quick snapshot of the absolute number of defaults (6636) and non-defaults (23364), as well as the proportion of defaults (22%). Almost a fourth of individuals from this dataset default. For training, I save the attribute data into a variable, `X`, where I remove the impertinent ID column, and the label data is saved in a variable, `y_true`. The first few rows of these dataset are outputted for confirmation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "id": "02b410a5-baf7-4b67-a303-b3de3179ba52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Default:     6636\n",
      "# Repaid:     23364\n",
      "% Defaulters:  0.22\n",
      "\n",
      "Attribute Data:\n",
      "\n",
      "   LIMIT_BAL  SEX  EDUCATION  MARRIAGE  AGE  PAY_0  PAY_2  PAY_3  PAY_4  \\\n",
      "0      20000    2          2         1   24      2      2     -1     -1   \n",
      "1     120000    2          2         2   26     -1      2      0      0   \n",
      "2      90000    2          2         2   34      0      0      0      0   \n",
      "3      50000    2          2         1   37      0      0      0      0   \n",
      "4      50000    1          2         1   57     -1      0     -1      0   \n",
      "\n",
      "   PAY_5  ...  BILL_AMT3  BILL_AMT4  BILL_AMT5  BILL_AMT6  PAY_AMT1  PAY_AMT2  \\\n",
      "0     -2  ...        689          0          0          0         0       689   \n",
      "1      0  ...       2682       3272       3455       3261         0      1000   \n",
      "2      0  ...      13559      14331      14948      15549      1518      1500   \n",
      "3      0  ...      49291      28314      28959      29547      2000      2019   \n",
      "4      0  ...      35835      20940      19146      19131      2000     36681   \n",
      "\n",
      "   PAY_AMT3  PAY_AMT4  PAY_AMT5  PAY_AMT6  \n",
      "0         0         0         0         0  \n",
      "1      1000      1000         0      2000  \n",
      "2      1000      1000      1000      5000  \n",
      "3      1200      1100      1069      1000  \n",
      "4     10000      9000       689       679  \n",
      "\n",
      "[5 rows x 23 columns]\n",
      "\n",
      "Label Data:\n",
      "\n",
      "0    1\n",
      "1    1\n",
      "2    0\n",
      "3    0\n",
      "4    0\n",
      "Name: default payment next month, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# import dataset\n",
    "data = pd.read_excel('./default of credit card clients.xls', header=1)\n",
    "num_default = data['default payment next month'].sum()\n",
    "num_no_default = data.shape[0] - num_default\n",
    "default_ratio = num_default / (num_no_default + num_default)\n",
    "print(f\"# Default: {num_default:>8}\\n# Repaid:  {num_no_default:>8}\\n% Defaulters: {default_ratio:>5.2f}\\n\")\n",
    "\n",
    "# assign variables\n",
    "X = data.iloc[:,1:-1] # removing ID column\n",
    "y_true = data['default payment next month']\n",
    "print(f\"Attribute Data:\\n\\n{X.head()}\\n\\nLabel Data:\\n\\n{y_true.head()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239fd373-505d-4e99-b6f8-6156b6ca7049",
   "metadata": {},
   "source": [
    "# Implementing ML Algorithms to Predict Defaults\n",
    "\n",
    "For my analysis, I take advantage of the scikit-learn library in Python. For all methods, I split my dataset into 80% training data and 20% test data. I chose not to work with a validation set; however, I implement certain methods that utilize built-in cross-validation, which breaks up the training dataset into mini validation sets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593ba514-2afb-44ad-b545-2b04c873ef6a",
   "metadata": {},
   "source": [
    "# Dummy Classifier (baseline)\n",
    "\n",
    "I start by implementing a Dummy Classifier to serve as a baseline method against which to compare other methods. The Dummy Classifier generates predictions at random, so we anticipate ~50% accuracy. I anchored the random number generator so the same results should be produced at each run. As expected, we see ~50% accuracy from the training dataset.\n",
    "\n",
    "I output a set of metrics for the test dataset, which includes accuracy, precision, recall, and F1-score. Accuracy for the test data is ~50%, as expected. For precision, we expect `(0.5x0.22)/0.5 = 0.22`, which we see in the data. The top of the equation reflects true positives (defaults) since random sampling gives 50% predicted-positive and our expectation of true positives reflects the population proportion (i.e., 22%); the bottom are the predicted-positives from our random labeling, which is 50%. For binary classification, I would not expect different precision values for macro and weighted averaging, which we see here.\n",
    "\n",
    "For recall, we expect `(0.5x0.22/0.22)`, which we see in the data. F1-score is `(2*0.22*0.5)/(0.22+0.5) = 0.31`, also reflected in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "id": "d211763f-417f-4946-a7f7-d28d1847578d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.4980 \n",
      "\n",
      "Test Metrics:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7755    0.5023    0.6097      4649\n",
      "           1     0.2258    0.4996    0.3111      1351\n",
      "\n",
      "    accuracy                         0.5017      6000\n",
      "   macro avg     0.5007    0.5009    0.4604      6000\n",
      "weighted avg     0.6517    0.5017    0.5424      6000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# split data\n",
    "X_train_d, X_test_d, y_train_d, y_test_d = train_test_split(X, y_true, test_size=0.2, random_state=25)\n",
    "\n",
    "# train\n",
    "baseline = DummyClassifier(strategy='uniform')\n",
    "baseline.fit(X_train_d,y_train_d)\n",
    "accuracy_train_d = baseline.score(X_train_d, y_train_d)\n",
    "print(f\"Training Accuracy: {accuracy_train_d:0.4f} \\n\")\n",
    "\n",
    "# test\n",
    "y_pred_d = baseline.predict(X_test_d)\n",
    "report_d = classification_report(y_test_d.to_numpy(), y_pred_d, digits=4)\n",
    "print(f\"Test Metrics:\\n {report_d}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a177379-986b-4d7c-ae9b-9b540759d5b7",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "I next try Logistic Regression to predict defaults. Data is preprocessed by standardizing features to zero mean and unit variance using `StandardScaler()` because features in some attributes had large variance relative to others. For the logistic regression algorithm, I used an L2 penalty because we have 30000 observations compared to 23 attributes, so I'm not looking to exclude attributes. I have also included the typical regularization step to address concerns of over-fitting.\n",
    "\n",
    "Since hyperparameters can have a large influence on outcomes, I used `LogisticRegressionCV` because it has built-in cross-validation to select the best value for the `C` hyperparameter, which inversely influences the strength of the regularization. I felt the default value of 5 was fine for the number of stratifications in cross-validation (i.e., the proportion of data in the validation set), as well as the default value of 10 for the number of `C` values to sample. I output the sampled `C` values and report the best `C` value (2.78) used in the final estimation.\n",
    "\n",
    "I then allow the algorithm to train and report a training accuracy of 0.81. Below I then report metrics for the test dataset, where we see similar accuracy (0.80). Precision is reasonable at 0.73; however, recall (or sensitivity, in binary classification) is poor (0.24). This implies that the current Logistic Regression approach has a higher chance of missing true defaulters, which is undesirable for banks. The model could be further tuned in an attempt to improve performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "id": "04ce25d9-b3ef-4869-91d3-021d1991becd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling C: [0.0001, 0.000774, 0.00599, 0.0464, 0.359, 2.78, 21.5, 167, 1.29e+03, 1e+04]\n",
      "Best C:      2.78\n",
      "\n",
      "Training Accuracy: 0.8113 \n",
      "\n",
      "Test Metrics:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8151    0.9750    0.8880      4649\n",
      "           1     0.7358    0.2391    0.3609      1351\n",
      "\n",
      "    accuracy                         0.8093      6000\n",
      "   macro avg     0.7755    0.6071    0.6244      6000\n",
      "weighted avg     0.7973    0.8093    0.7693      6000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# split data\n",
    "X_train_l, X_test_l, y_train_l, y_test_l = train_test_split(X, y_true, test_size=0.2, random_state=25)\n",
    "\n",
    "# preprocess data and train\n",
    "pipe_l = make_pipeline(StandardScaler(), LogisticRegressionCV(random_state=25)) # default: L2 penalty, Stratified 5-Fold for cross-validation\n",
    "X_scaled = pipe_l.fit(X_train_l, y_train_l)\n",
    "accuracy_train_l = pipe_l.score(X_train_l, y_train_l)\n",
    "\n",
    "# check C hyperparameter\n",
    "grab_log_reg = pipe_l.named_steps['logisticregressioncv']\n",
    "Cs_values = grab_log_reg.Cs_\n",
    "coefficients = grab_log_reg.coefs_paths_\n",
    "scores_C = grab_log_reg.scores_\n",
    "best_C = grab_log_reg.C_\n",
    "\n",
    "def sig_figs(x, sigs):\n",
    "    return \"{:.{p}g}\".format(x, p=sigs)\n",
    "C_vals_sampled = \", \".join(sig_figs(x, 3) for x in Cs_values)\n",
    "print(f\"Sampling C: [{C_vals_sampled}]\")\n",
    "print(f\"Best C:      {C_value[0]:0.2f}\\n\")\n",
    "print(f\"Training Accuracy: {accuracy_train_l:0.4f} \\n\")\n",
    "\n",
    "# test\n",
    "y_pred_l = pipe_l.predict(X_test_l)\n",
    "report_l = classification_report(y_test_l, y_pred_l, digits=4)\n",
    "print(f\"Test Metrics:\\n {report_l}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f481c3-cea0-408b-a232-43254721de9f",
   "metadata": {},
   "source": [
    "# Support Vector Classification (SVC)\n",
    "\n",
    "Another classic approach - Support Vector Machines (specifically SVC for this problem). Again, I'm interested in sampling multiple hyperparameter values and using cross-validation to identify the best set, where cross-validation is built in to `GridSearchCV`. I'm opting to compare a linear kernel and an rbf kernel for the transformation portion of SVM because linear is simple and rbf is commonly used. I'm testing different `C` values but opting to use default for `gamma` (kernel coefficient) because it is adaptable to the dataset, i.e., `1 / (n_features * average_variance_across_features)`. As in my Logistic Regression approach, I am also standardizing the features to zero mean and unit variance.\n",
    "\n",
    "Waiting for code to run..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12df85f2-8ec0-43d8-8882-aeee12ff2a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data\n",
    "X_train_s, X_test_s, y_train_s, y_test_s = train_test_split(X, y_true, test_size=0.2, random_state=25)\n",
    "\n",
    "# hyperparameter tuning and cross-validation\n",
    "hyperparam_sampling = {\n",
    "    'svc__C': [0.1, 1, 10, 100],\n",
    "    'svc__kernel': ['linear', 'rbf']\n",
    "}\n",
    "pipe_s = make_pipeline(StandardScaler(), SVC(random_state=25))\n",
    "tuning_s = GridSearchCV(pipe_s, hyperparam_sampling) # default: Stratified 5-Fold for cross-validation\n",
    "\n",
    "# train\n",
    "tuning_s.fit(X_train_s, y_train_s)\n",
    "best_hyperparams = tuning_s.best_params_\n",
    "best_estimator = tuning_s.best_estimator_\n",
    "print(f\"Best parameters: {best_hyperparams}\")\n",
    "print(f\"Best estimator: {best_estimator}\")\n",
    "\n",
    "# Test the model\n",
    "y_pred_s = tuning_s.predict(X_test_s)\n",
    "print(\"Test Metrics:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f17bf1b-244b-436f-b894-20882d4ffea1",
   "metadata": {},
   "source": [
    "# Random Forest\n",
    "\n",
    "I went for Random Forest over Decision Trees because I assumed I have enough computing power for a Random Forest, which performs better than a Decision Tree with less for over-fitting. I chose to explore three hyperparameters (`n_estimators`, `max_depth`, and `min_samples_split`) and again used cross-validation built in to `GridSearchCV`.\n",
    "\n",
    "Waiting for code to run..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a2cc30-1d8d-48b2-bffd-e702ec62d320",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data\n",
    "X_train_r, X_test_r, y_train_r, y_test_r = train_test_split(X, y_true, test_size=0.2, random_state=25)\n",
    "\n",
    "# hyperparameter tuning and cross-validation\n",
    "hyperparam_sampling = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "tuning_r = GridSearchCV(estimator=RandomForestClassifier(random_state=25), param_grid=hyperparam_sampling) # default: Stratified 5-Fold for cross-validation\n",
    "\n",
    "# train\n",
    "tuning_r.fit(X_train_r, y_train_r)\n",
    "best_hyperparams = tuning_r.best_params_\n",
    "print(best_hyperparams)\n",
    "\n",
    "# test\n",
    "y_pred_r = tuning_r.predict(X_test_r)\n",
    "report_r = classification_report(y_test_r, y_pred_r, digits=4)\n",
    "print(f\"Test Metrics:\\n {report_r}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0087f5aa-1480-4a6d-a489-f3e2e5539d23",
   "metadata": {},
   "source": [
    "<h4>For all of these methods, I could dive further into the weeds by exploring colinearity of attributes, dimensionality reduction, etc. And better yet, I could explore even more machine learning algorithms.</h4>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
